import argparse
import csv
import os
import sqlite3
import statistics
import time
from pathlib import Path

# OFFICIAL KUKANILEA DB BENCHMARK
# SOURCE: https://www.sqlite.org/pragma.html
# WAL + synchronous=NORMAL is consistent but may lose durability on power loss.

def setup_db(db_path):
    if os.path.exists(db_path):
        os.remove(db_path)
    
    conn = sqlite3.connect(db_path)
    conn.execute("PRAGMA journal_mode=WAL;")
    conn.execute("PRAGMA synchronous=NORMAL;")
    conn.execute("PRAGMA busy_timeout=5000;")
    
    conn.execute("CREATE TABLE benchmark (id INTEGER PRIMARY KEY, val TEXT);")
    conn.commit()
    return conn

def benchmark_batch(conn, rows):
    """Mode A: one transaction (BEGIN; many inserts; COMMIT)"""
    start_time = time.perf_counter()
    data = [(f"data_{i}",) for i in range(rows)]
    conn.executemany("INSERT INTO benchmark (val) VALUES (?);", data)
    conn.commit()
    duration = time.perf_counter() - start_time
    return duration

def benchmark_single(conn, rows):
    """Mode B: many transactions (commit per row)"""
    start_time = time.perf_counter()
    latencies = []
    for i in range(rows):
        t0 = time.perf_counter()
        conn.execute("INSERT INTO benchmark (val) VALUES (?);", (f"single_{i}",))
        conn.commit()
        latencies.append((time.perf_counter() - t0) * 1000)
    
    duration = time.perf_counter() - start_time
    return duration, latencies

def main():
    parser = argparse.ArgumentParser(description="KUKANILEA SQLite Benchmark")
    parser.add_argument("--db", default="test_bench.db", help="Database path")
    parser.add_argument("--rows", type=int, default=1000, help="Number of rows")
    parser.add_argument("--out", default="evidence/bench", help="Output directory")
    args = parser.parse_args()

    out_dir = Path(args.out)
    out_dir.mkdir(parents=True, exist_ok=True)

    conn = setup_db(args.db)
    
    # Mode A
    batch_dur = benchmark_batch(conn, args.rows)
    batch_tput = args.rows / batch_dur

    # Mode B
    single_dur, single_latencies = benchmark_single(conn, args.rows)
    single_tput = args.rows / single_dur
    p50 = statistics.median(single_latencies)
    p95 = statistics.quantiles(single_latencies, n=20)[18] if len(single_latencies) >= 20 else p50
    p99 = statistics.quantiles(single_latencies, n=100)[98] if len(single_latencies) >= 100 else p50

    # Report
    report = f"""# SQLite DB Benchmark Report (KUKANILEA)

## Configuration
- `journal_mode`: WAL
- `synchronous`: NORMAL
- `busy_timeout`: 5000ms

> [!IMPORTANT]
> **Compliance Note:** According to [sqlite.org](https://www.sqlite.org/pragma.html), WAL mode with `synchronous=NORMAL` is consistent (prevents corruption) but may lose durability across a power loss.

## Results ({args.rows} rows)

### Mode A: Batch Transaction (High Throughput)
- Total Duration: {batch_dur:.4f}s
- Throughput: {batch_tput:.2f} ops/sec

### Mode B: Single Transaction (Commit per Row)
- Total Duration: {single_dur:.4f}s
- Throughput: {single_tput:.2f} ops/sec
- Latency (ms):
    - p50: {p50:.4f}
    - p95: {p95:.4f}
    - p99: {p99:.4f}

---
*Generated by scripts/benchmark_db.py*
"""
    (out_dir / "REPORT_DB_BENCHMARK.md").write_text(report)
    
    # CSV
    with open(out_dir / "db_bench.csv", "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(["mode", "rows", "duration_s", "throughput_ops_sec", "p50_ms"])
        writer.writerow(["batch", args.rows, f"{batch_dur:.4f}", f"{batch_tput:.2f}", "N/A"])
        writer.writerow(["single", args.rows, f"{single_dur:.4f}", f"{single_tput:.2f}", f"{p50:.4f}"])

    print(f"Benchmark complete. Evidence: {out_dir}")
    conn.close()

if __name__ == "__main__":
    main()
